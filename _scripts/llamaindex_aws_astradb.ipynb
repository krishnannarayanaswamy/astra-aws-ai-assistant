{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using llama-parse with AstraDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we show a basic RAG-style example that uses `llama-parse` to parse a PDF document, store the corresponding document into a vector store (`AstraDB`) and finally, perform some basic queries against that store. The notebook is modeled after the quick start notebooks and hence is meant as a way of getting started with `llama-parse`, backed by a vector database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.10/site-packages (1.34.66)\n",
      "Requirement already satisfied: llama-index in /usr/local/lib/python3.10/site-packages (0.9.48)\n",
      "Requirement already satisfied: llama-index-core==0.10.6.post1 in /usr/local/lib/python3.10/site-packages (0.10.6.post1)\n",
      "Requirement already satisfied: llama-parse in /usr/local/lib/python3.10/site-packages (0.1.4)\n",
      "Requirement already satisfied: llama-index-vector-stores-astra-db in /usr/local/lib/python3.10/site-packages (0.1.5)\n",
      "Requirement already satisfied: llama-index-llms-bedrock in /usr/local/lib/python3.10/site-packages (0.1.5)\n",
      "Requirement already satisfied: llama-index-embeddings-bedrock in /usr/local/lib/python3.10/site-packages (0.1.4)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/site-packages (from llama-index-core==0.10.6.post1) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.10/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core==0.10.6.post1) (2.0.25)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/site-packages (from llama-index-core==0.10.6.post1) (3.9.1)\n",
      "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/site-packages (from llama-index-core==0.10.6.post1) (0.5.14)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/site-packages (from llama-index-core==0.10.6.post1) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/site-packages (from llama-index-core==0.10.6.post1) (1.0.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/site-packages (from llama-index-core==0.10.6.post1) (2023.12.2)\n",
      "Requirement already satisfied: httpx in /usr/local/lib/python3.10/site-packages (from llama-index-core==0.10.6.post1) (0.25.2)\n",
      "Requirement already satisfied: llamaindex-py-client>=0.1.12 in /usr/local/lib/python3.10/site-packages (from llama-index-core==0.10.6.post1) (0.1.13)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/site-packages (from llama-index-core==0.10.6.post1) (1.5.9)\n",
      "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/site-packages (from llama-index-core==0.10.6.post1) (3.2.1)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/site-packages (from llama-index-core==0.10.6.post1) (3.8.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/site-packages (from llama-index-core==0.10.6.post1) (1.26.3)\n",
      "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.10/site-packages (from llama-index-core==0.10.6.post1) (1.11.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/site-packages (from llama-index-core==0.10.6.post1) (2.1.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/site-packages (from llama-index-core==0.10.6.post1) (10.2.0)\n",
      "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/site-packages (from llama-index-core==0.10.6.post1) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/site-packages (from llama-index-core==0.10.6.post1) (8.2.3)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/site-packages (from llama-index-core==0.10.6.post1) (0.5.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/site-packages (from llama-index-core==0.10.6.post1) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/site-packages (from llama-index-core==0.10.6.post1) (4.9.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/site-packages (from llama-index-core==0.10.6.post1) (0.9.0)\n",
      "Requirement already satisfied: botocore<1.35.0,>=1.34.66 in /usr/local/lib/python3.10/site-packages (from boto3) (1.34.66)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /usr/local/lib/python3.10/site-packages (from boto3) (0.10.1)\n",
      "Requirement already satisfied: astrapy<0.8.0,>=0.7.7 in /usr/local/lib/python3.10/site-packages (from llama-index-vector-stores-astra-db) (0.7.7)\n",
      "Requirement already satisfied: llama-index-llms-anthropic<0.2.0,>=0.1.7 in /usr/local/lib/python3.10/site-packages (from llama-index-llms-bedrock) (0.1.7)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.6.post1) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.6.post1) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.6.post1) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.6.post1) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.6.post1) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.6.post1) (4.0.3)\n",
      "Requirement already satisfied: cassio<0.2.0,>=0.1.4 in /usr/local/lib/python3.10/site-packages (from astrapy<0.8.0,>=0.7.7->llama-index-vector-stores-astra-db) (0.1.5)\n",
      "Requirement already satisfied: deprecation<2.2.0,>=2.1.0 in /usr/local/lib/python3.10/site-packages (from astrapy<0.8.0,>=0.7.7->llama-index-vector-stores-astra-db) (2.1.0)\n",
      "Requirement already satisfied: toml<0.11.0,>=0.10.2 in /usr/local/lib/python3.10/site-packages (from astrapy<0.8.0,>=0.7.7->llama-index-vector-stores-astra-db) (0.10.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/site-packages (from botocore<1.35.0,>=1.34.66->boto3) (2.8.2)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.10/site-packages (from botocore<1.35.0,>=1.34.66->boto3) (2.1.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/site-packages (from deprecated>=1.2.9.3->llama-index-core==0.10.6.post1) (1.16.0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.10/site-packages (from httpx->llama-index-core==0.10.6.post1) (3.7.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/site-packages (from httpx->llama-index-core==0.10.6.post1) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/site-packages (from httpx->llama-index-core==0.10.6.post1) (1.0.2)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.10/site-packages (from httpx->llama-index-core==0.10.6.post1) (3.6)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/site-packages (from httpx->llama-index-core==0.10.6.post1) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/site-packages (from httpcore==1.*->httpx->llama-index-core==0.10.6.post1) (0.14.0)\n",
      "Requirement already satisfied: anthropic<0.21.0,>=0.20.0 in /usr/local/lib/python3.10/site-packages (from llama-index-llms-anthropic<0.2.0,>=0.1.7->llama-index-llms-bedrock) (0.20.0)\n",
      "Requirement already satisfied: pydantic>=1.10 in /usr/local/lib/python3.10/site-packages (from llamaindex-py-client>=0.1.12->llama-index-core==0.10.6.post1) (2.5.3)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core==0.10.6.post1) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core==0.10.6.post1) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core==0.10.6.post1) (2023.12.25)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/site-packages (from openai>=1.1.0->llama-index-core==0.10.6.post1) (1.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests>=2.31.0->llama-index-core==0.10.6.post1) (3.3.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core==0.10.6.post1) (3.0.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/site-packages (from typing-inspect>=0.8.0->llama-index-core==0.10.6.post1) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/site-packages (from dataclasses-json->llama-index-core==0.10.6.post1) (3.20.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/site-packages (from pandas->llama-index-core==0.10.6.post1) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/site-packages (from pandas->llama-index-core==0.10.6.post1) (2023.4)\n",
      "Requirement already satisfied: tokenizers>=0.13.0 in /usr/local/lib/python3.10/site-packages (from anthropic<0.21.0,>=0.20.0->llama-index-llms-anthropic<0.2.0,>=0.1.7->llama-index-llms-bedrock) (0.15.1)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/site-packages (from anyio->httpx->llama-index-core==0.10.6.post1) (1.2.0)\n",
      "Requirement already satisfied: cassandra-driver>=3.28.0 in /usr/local/lib/python3.10/site-packages (from cassio<0.2.0,>=0.1.4->astrapy<0.8.0,>=0.7.7->llama-index-vector-stores-astra-db) (3.28.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/site-packages (from deprecation<2.2.0,>=2.1.0->astrapy<0.8.0,>=0.7.7->llama-index-vector-stores-astra-db) (23.2)\n",
      "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.10/site-packages (from httpx[http2]<1,>=0.25.2->astrapy<0.8.0,>=0.7.7->llama-index-vector-stores-astra-db) (4.1.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/site-packages (from pydantic>=1.10->llamaindex-py-client>=0.1.12->llama-index-core==0.10.6.post1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /usr/local/lib/python3.10/site-packages (from pydantic>=1.10->llamaindex-py-client>=0.1.12->llama-index-core==0.10.6.post1) (2.14.6)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.66->boto3) (1.16.0)\n",
      "Requirement already satisfied: geomet<0.3,>=0.1 in /usr/local/lib/python3.10/site-packages (from cassandra-driver>=3.28.0->cassio<0.2.0,>=0.1.4->astrapy<0.8.0,>=0.7.7->llama-index-vector-stores-astra-db) (0.2.1.post1)\n",
      "Requirement already satisfied: hyperframe<7,>=6.0 in /usr/local/lib/python3.10/site-packages (from h2<5,>=3->httpx[http2]<1,>=0.25.2->astrapy<0.8.0,>=0.7.7->llama-index-vector-stores-astra-db) (6.0.1)\n",
      "Requirement already satisfied: hpack<5,>=4.0 in /usr/local/lib/python3.10/site-packages (from h2<5,>=3->httpx[http2]<1,>=0.25.2->astrapy<0.8.0,>=0.7.7->llama-index-vector-stores-astra-db) (4.0.0)\n",
      "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/site-packages (from tokenizers>=0.13.0->anthropic<0.21.0,>=0.20.0->llama-index-llms-anthropic<0.2.0,>=0.1.7->llama-index-llms-bedrock) (0.20.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.21.0,>=0.20.0->llama-index-llms-anthropic<0.2.0,>=0.1.7->llama-index-llms-bedrock) (3.13.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# First, install the required dependencies\n",
    "!pip install boto3 llama-index llama-index-core==0.10.6.post1 llama-parse llama-index-vector-stores-astra-db llama-index-llms-bedrock llama-index-embeddings-bedrock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Get all required API keys and parameters\n",
    "llama_cloud_api_key = getpass(\"Enter your Llama Index Cloud API Key: \")\n",
    "api_endpoint = input(\"Enter your Astra DB API Endpoint: \")\n",
    "token = getpass(\"Enter your Astra DB Token: \")\n",
    "namespace = input(\"Enter your Astra DB namespace (optional, must exist on Astra): \") or None\n",
    "aws_access_key = getpass(\"Enter your AWS Access Key: \")\n",
    "aws_secret_access_key = getpass(\"Enter your AWS Secret Access Key: \")\n",
    "aws_session_key = getpass(\"Enter your AWS Session Key: \")\n",
    "\n",
    "os.environ[\"LLAMA_CLOUD_API_KEY\"] = llama_cloud_api_key\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = aws_access_key\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = aws_secret_access_key\n",
    "os.environ[\"AWS_SESSION_TOKEN\"] = aws_session_key\n",
    "\n",
    "os.environ['ASTRA_DB_APPLICATION_TOKEN'] = token\n",
    "os.environ['ASTRA_DB_API_ENDPOINT'] = api_endpoint\n",
    "os.environ['ASTRA_DB_KEYSPACE'] = namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama-parse is async-first, running the sync code in a notebook requires the use of nest_asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using llama-parse to parse a PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete.\n"
     ]
    }
   ],
   "source": [
    "# Grab a PDF from Arxiv for indexing\n",
    "import requests \n",
    "\n",
    "# The URL of the file you want to download\n",
    "url = \"https://arxiv.org/pdf/1706.03762.pdf\"\n",
    "# The local path where you want to save the file\n",
    "file_path = \"file.pdf\"\n",
    "\n",
    "# Perform the HTTP request\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Open the file in binary write mode and save the content\n",
    "    with open(file_path, \"wb\") as file:\n",
    "        file.write(response.content)\n",
    "    print(\"Download complete.\")\n",
    "else:\n",
    "    print(\"Error downloading the file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id f396b7b0-f612-4aad-adb0-91d185f3016f\n"
     ]
    }
   ],
   "source": [
    "from llama_parse import LlamaParse\n",
    "\n",
    "documents = LlamaParse(result_type=\"text\").load_data(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a quick look at some of the parsed text from the document:\n",
    "documents[0].get_content()[3000:5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing into Astra DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.astra_db import AstraDBVectorStore\n",
    "\n",
    "astra_db_store = AstraDBVectorStore(\n",
    "    token=token,\n",
    "    api_endpoint=api_endpoint,\n",
    "    namespace=namespace,\n",
    "    collection_name=\"aws_astra_assistant\",\n",
    "    embedding_dimension=1024\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SimpleNodeParser\n",
    "\n",
    "node_parser = SimpleNodeParser()\n",
    "\n",
    "nodes = node_parser.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.bedrock import Bedrock\n",
    "from llama_index.embeddings.bedrock import BedrockEmbedding, Models\n",
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=astra_db_store)\n",
    "\n",
    "index = VectorStoreIndex(\n",
    "    nodes=nodes,\n",
    "    storage_context=storage_context,\n",
    "    embed_model=BedrockEmbedding(model = \"cohere.embed-multilingual-v3\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple RAG Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(similarity_top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***********New LlamaParse+ Basic Query Engine***********\n",
      "Multi-Head Attention is also known as multi-headed self-attention.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is Multi-Head Attention also known as?\"\n",
    "\n",
    "response_1 = query_engine.query(query)\n",
    "print(\"\\n***********New LlamaParse+ Basic Query Engine***********\")\n",
    "print(response_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n6.3    English Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n                                                             9\\n---\\nTable 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\n                          Parser                           Training             WSJ 23 F1\\n            Vinyals & Kaiser el al. (2014) [37]    WSJ only, discriminative         88.3\\n                 Petrov et al. (2006) [29]         WSJ only, discriminative         90.4\\n                   Zhu et al. (2013) [40]          WSJ only, discriminative         90.4\\n                   Dyer et al. (2016) [8]          WSJ only, discriminative         91.7\\n                  Transformer (4 layers)           WSJ only, discriminative         91.3\\n                   Zhu et al. (2013) [40]              semi-supervised              91.3\\n               Huang & Harper (2009) [14]              semi-supervised              91.3\\n               McClosky et al. (2006) [26]             semi-supervised              92.1\\n            Vinyals & Kaiser el al. (2014) [37]        semi-supervised              92.1\\n                  Transformer (4 layers)               semi-supervised              92.7\\n                 Luong et al. (2015) [23]                  multi-task               93.0\\n                   Dyer et al. (2016) [8]                  generative               93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and Î± = 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7    Conclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a look at one of the source nodes from the response\n",
    "response_1.source_nodes[0].get_content()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
